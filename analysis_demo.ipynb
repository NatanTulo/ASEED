{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d1b5d1",
   "metadata": {},
   "source": [
    "# ASEED - Analiza ZamÃ³wieÅ„ E-commerce w Czasie Rzeczywistym\n",
    "\n",
    "## System analizy zamÃ³wieÅ„ sklepu online z uÅ¼yciem Kafka + Spark Structured Streaming\n",
    "\n",
    "ðŸŽ¯ **Cel projektu**: Zbudowanie systemu analizujÄ…cego zamÃ³wienia e-commerce w czasie rzeczywistym\n",
    "\n",
    "**Architektura**:\n",
    "- **Order Simulator** â†’ generuje zamÃ³wienia (order_id, product_id, price, timestamp)\n",
    "- **Apache Kafka** â†’ przesyÅ‚a dane strumieniowo\n",
    "- **Spark Structured Streaming** â†’ analizuje top produkty w czasie rzeczywistym  \n",
    "- **Web Dashboard** â†’ wizualizuje wyniki\n",
    "\n",
    "**Wymagania speÅ‚nione**:\n",
    "- âœ… Kafka topic z zamÃ³wieniami (order_id, product_id, price, timestamp)\n",
    "- âœ… Spark Structured Streaming do agregacji\n",
    "- âœ… Praktyczne wzorce ETL i streaming aggregations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a70b7",
   "metadata": {},
   "source": [
    "## 1. Symulacja danych zamÃ³wieÅ„ i wysyÅ‚ka do Kafki\n",
    "\n",
    "W tej sekcji zademonstrujemy jak generowaÄ‡ przykÅ‚adowe dane zamÃ³wieÅ„ i wysyÅ‚aÄ‡ je do tematu Kafka.\n",
    "\n",
    "**Schema zamÃ³wienia**:\n",
    "```json\n",
    "{\n",
    "  \"order_id\": \"string\",\n",
    "  \"product_id\": \"string\", \n",
    "  \"price\": \"float\",\n",
    "  \"timestamp\": \"datetime\",\n",
    "  \"quantity\": \"int\",\n",
    "  \"category\": \"string\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198df5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy i konfiguracja\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "\n",
    "# PrzykÅ‚adowe produkty (zgodne z ASEED)\n",
    "PRODUCTS = [\n",
    "    {'id': 'PROD-001', 'name': 'Smart Watch Premium', 'category': 'Electronics', 'price': 299.99},\n",
    "    {'id': 'PROD-002', 'name': 'Fashion Jacket', 'category': 'Clothing', 'price': 89.99},\n",
    "    {'id': 'PROD-003', 'name': 'Python Programming Book', 'category': 'Books', 'price': 45.99},\n",
    "    {'id': 'PROD-004', 'name': 'Coffee Machine Pro', 'category': 'Home', 'price': 199.99},\n",
    "    {'id': 'PROD-005', 'name': 'Running Shoes', 'category': 'Sports', 'price': 129.99}\n",
    "]\n",
    "\n",
    "def generate_order():\n",
    "    \"\"\"Generuje pojedyncze zamÃ³wienie zgodnie ze schematem\"\"\"\n",
    "    product = random.choice(PRODUCTS)\n",
    "    quantity = random.randint(1, 3)\n",
    "    \n",
    "    order = {\n",
    "        'order_id': f'ORD-{random.randint(100000, 999999)}',\n",
    "        'product_id': product['id'],\n",
    "        'product_name': product['name'],\n",
    "        'category': product['category'],\n",
    "        'price': product['price'],\n",
    "        'quantity': quantity,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return order\n",
    "\n",
    "# PrzykÅ‚ad wygenerowanego zamÃ³wienia\n",
    "sample_order = generate_order()\n",
    "print(\"PrzykÅ‚ad zamÃ³wienia:\")\n",
    "print(json.dumps(sample_order, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ba2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja producenta Kafka (DEMO - bez rzeczywistego uruchomienia)\n",
    "def create_kafka_producer():\n",
    "    \"\"\"Tworzy producenta Kafka do wysyÅ‚ania zamÃ³wieÅ„\"\"\"\n",
    "    try:\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=['localhost:9092'],\n",
    "            value_serializer=lambda x: json.dumps(x, default=str).encode('utf-8'),\n",
    "            key_serializer=lambda x: x.encode('utf-8') if x else None\n",
    "        )\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Kafka niedostÄ™pna: {e}\")\n",
    "        print(\"ðŸ’¡ Aby uruchomiÄ‡ Kafka, uÅ¼yj: python3 ../aseed.py start\")\n",
    "        return None\n",
    "\n",
    "def send_orders_to_kafka(producer, topic='orders', count=10):\n",
    "    \"\"\"WysyÅ‚a zamÃ³wienia do tematu Kafka\"\"\"\n",
    "    if not producer:\n",
    "        print(\"ðŸ”„ Symulacja wysyÅ‚ania do Kafka (bez rzeczywistego wysyÅ‚ania):\")\n",
    "        \n",
    "    orders_sent = []\n",
    "    for i in range(count):\n",
    "        order = generate_order()\n",
    "        orders_sent.append(order)\n",
    "        \n",
    "        if producer:\n",
    "            # Rzeczywiste wysyÅ‚anie\n",
    "            producer.send(topic, key=order['order_id'], value=order)\n",
    "            print(f\"âœ… WysÅ‚ano zamÃ³wienie {i+1}/{count}: {order['product_name']}\")\n",
    "        else:\n",
    "            # Symulacja\n",
    "            print(f\"ðŸ“ [{i+1}/{count}] {order['order_id']}: {order['product_name']} (${order['price']})\")\n",
    "        \n",
    "        time.sleep(0.1)  # OpÃ³Åºnienie miÄ™dzy zamÃ³wieniami\n",
    "    \n",
    "    if producer:\n",
    "        producer.flush()\n",
    "        print(f\"\\nðŸš€ WysÅ‚ano {count} zamÃ³wieÅ„ do tematu '{topic}'\")\n",
    "    \n",
    "    return orders_sent\n",
    "\n",
    "# Demonstracja (bez rzeczywistego poÅ‚Ä…czenia)\n",
    "print(\"=== DEMONSTRACJA WYSYÅANIA ZAMÃ“WIEÅƒ DO KAFKA ===\")\n",
    "demo_producer = create_kafka_producer()\n",
    "sample_orders = send_orders_to_kafka(demo_producer, count=5)\n",
    "\n",
    "# WyÅ›wietlenie statystyk\n",
    "df_orders = pd.DataFrame(sample_orders)\n",
    "print(f\"\\nðŸ“Š Statystyki wygenerowanych zamÃ³wieÅ„:\")\n",
    "print(f\"- ÅÄ…czna liczba: {len(df_orders)}\")\n",
    "print(f\"- Kategorie: {df_orders['category'].value_counts().to_dict()}\")\n",
    "print(f\"- Åšrednia cena: ${df_orders['price'].mean():.2f}\")\n",
    "print(f\"- ÅÄ…czna wartoÅ›Ä‡: ${(df_orders['price'] * df_orders['quantity']).sum():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee712a9f",
   "metadata": {},
   "source": [
    "## 2. Konfiguracja Spark Structured Streaming do odbioru danych z Kafki\n",
    "\n",
    "Spark Structured Streaming umoÅ¼liwia przetwarzanie strumieni danych w czasie rzeczywistym z wysokÄ… wydajnoÅ›ciÄ… i fault-tolerance.\n",
    "\n",
    "**Kluczowe koncepcje**:\n",
    "- **DataFrame API** - deklaratywne API do streamingu\n",
    "- **Checkpointing** - odpornoÅ›Ä‡ na awarie  \n",
    "- **Watermarking** - obsÅ‚uga spÃ³Åºnionych danych\n",
    "- **Trigger** - kontrola czÄ™stotliwoÅ›ci przetwarzania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91292b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja Spark Session (DEMO)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Tworzy Spark Session z odpowiedniÄ… konfiguracjÄ…\"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ASEED-OrderAnalysis\") \\\n",
    "            .config(\"spark.sql.streaming.checkpointLocation\", \"./checkpoints\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        spark.sparkContext.setLogLevel(\"WARN\")  # Zmniejszenie iloÅ›ci logÃ³w\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  BÅ‚Ä…d tworzenia Spark Session: {e}\")\n",
    "        print(\"ðŸ’¡ W Å›rodowisku ASEED Spark jest automatycznie skonfigurowany\")\n",
    "        return None\n",
    "\n",
    "# Schema dla zamÃ³wieÅ„ (zgodna z formatem ASEED)\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True), \n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"ðŸ“‹ Schema zamÃ³wienia:\")\n",
    "for field in order_schema.fields:\n",
    "    print(f\"  - {field.name}: {field.dataType} (nullable: {field.nullable})\")\n",
    "\n",
    "# Demonstracja konfiguracji (bez rzeczywistego Spark)\n",
    "print(\"\\nðŸ”§ Konfiguracja Spark Structured Streaming:\")\n",
    "print(\"- Application: ASEED-OrderAnalysis\")\n",
    "print(\"- Checkpoint: ./checkpoints\") \n",
    "print(\"- Adaptive Query Execution: Enabled\")\n",
    "print(\"- Log Level: WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja odczytu z Kafka \n",
    "def create_kafka_stream(spark, kafka_servers=\"localhost:9092\", topic=\"orders\"):\n",
    "    \"\"\"Tworzy streaming DataFrame z Kafki\"\"\"\n",
    "    if not spark:\n",
    "        print(\"âš ï¸  Brak Spark Session - pokazujemy kod konfiguracyjny:\")\n",
    "        \n",
    "    kafka_options = {\n",
    "        \"kafka.bootstrap.servers\": kafka_servers,\n",
    "        \"subscribe\": topic,\n",
    "        \"startingOffsets\": \"latest\",  # Zaczynamy od najnowszych wiadomoÅ›ci\n",
    "        \"failOnDataLoss\": \"false\"     # Kontynuuj mimo utraty danych\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ“¡ Konfiguracja odczytu z Kafka:\")\n",
    "    for key, value in kafka_options.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    if not spark:\n",
    "        return None\n",
    "        \n",
    "    # Streaming DataFrame z Kafki\n",
    "    kafka_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .options(**kafka_options) \\\n",
    "        .load()\n",
    "    \n",
    "    # Parsing JSON z wartoÅ›ci Kafka\n",
    "    orders_df = kafka_df.select(\n",
    "        col(\"key\").cast(\"string\").alias(\"order_key\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"order\")\n",
    "    ).select(\"order_key\", \"order.*\")\n",
    "    \n",
    "    # Konwersja timestamp do wÅ‚aÅ›ciwego typu\n",
    "    orders_df = orders_df.withColumn(\n",
    "        \"timestamp\", \n",
    "        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\")\n",
    "    )\n",
    "    \n",
    "    return orders_df\n",
    "\n",
    "# Demonstracja (bez rzeczywistego Spark)\n",
    "print(\"=== KONFIGURACJA STREAMING DATAFRAME ===\")\n",
    "demo_stream = create_kafka_stream(None)\n",
    "\n",
    "print(\"\\nðŸ”„ Logiczny plan odczytu z Kafki:\")\n",
    "print(\"1. PoÅ‚Ä…czenie z Kafka (localhost:9092)\")\n",
    "print(\"2. Subskrybcja tematu 'orders'\") \n",
    "print(\"3. Parsing JSON z kolumny 'value'\")\n",
    "print(\"4. Konwersja timestamp do wÅ‚aÅ›ciwego typu\")\n",
    "print(\"5. Utworzenie streaming DataFrame z kolumnami:\")\n",
    "print(\"   - order_key, order_id, product_id, product_name\")\n",
    "print(\"   - category, price, quantity, timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60243252",
   "metadata": {},
   "source": [
    "## 3. Agregacje strumieniowe: najpopularniejsze produkty\n",
    "\n",
    "**Streaming Aggregations** to kluczowa funkcja Spark Structured Streaming pozwalajÄ…ca na:\n",
    "- Obliczanie metryk w czasie rzeczywistym\n",
    "- Okna czasowe (tumbling, sliding, session windows)\n",
    "- Watermarking dla spÃ³Åºnionych danych\n",
    "- Stateful operations z checkpointingiem\n",
    "\n",
    "**Przypadki uÅ¼ycia w ASEED**:\n",
    "- Top produkty wedÅ‚ug liczby zamÃ³wieÅ„\n",
    "- Przychody wedÅ‚ug kategorii\n",
    "- Trendy sprzedaÅ¼y w oknie czasowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d414ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcje agregacji dla top produktÃ³w\n",
    "def create_top_products_aggregation(orders_df):\n",
    "    \"\"\"Tworzy agregacjÄ™ top produktÃ³w w czasie rzeczywistym\"\"\"\n",
    "    if not orders_df:\n",
    "        print(\"âš ï¸  Brak streaming DataFrame - pokazujemy logikÄ™ agregacji:\")\n",
    "        print(\"\"\"\n",
    "        # Top produkty - agregacja grupowa  \n",
    "        top_products = orders_df \\\\\n",
    "            .withWatermark(\"timestamp\", \"10 minutes\") \\\\\n",
    "            .groupBy(\"product_id\", \"product_name\", \"category\") \\\\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"order_count\"),\n",
    "                sum(\"quantity\").alias(\"total_quantity\"), \n",
    "                sum(col(\"price\") * col(\"quantity\")).alias(\"total_revenue\"),\n",
    "                max(\"timestamp\").alias(\"last_order_time\")\n",
    "            ) \\\\\n",
    "            .orderBy(desc(\"order_count\"))\n",
    "        \"\"\")\n",
    "        return None\n",
    "        \n",
    "    # Rzeczywista agregacja (gdyby byÅ‚ DataFrame)\n",
    "    top_products = orders_df \\\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "        .groupBy(\"product_id\", \"product_name\", \"category\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            sum(\"quantity\").alias(\"total_quantity\"),\n",
    "            sum(col(\"price\") * col(\"quantity\")).alias(\"total_revenue\"),\n",
    "            max(\"timestamp\").alias(\"last_order_time\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"order_count\"))\n",
    "    \n",
    "    return top_products\n",
    "\n",
    "def create_category_aggregation(orders_df):\n",
    "    \"\"\"Agregacja przychodÃ³w wedÅ‚ug kategorii\"\"\"\n",
    "    if not orders_df:\n",
    "        print(\"\"\"\n",
    "        # Przychody wedÅ‚ug kategorii\n",
    "        category_revenue = orders_df \\\\\n",
    "            .withWatermark(\"timestamp\", \"5 minutes\") \\\\\n",
    "            .groupBy(\"category\") \\\\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"total_orders\"),\n",
    "                sum(col(\"price\") * col(\"quantity\")).alias(\"total_revenue\"),\n",
    "                avg(\"price\").alias(\"avg_price\")\n",
    "            ) \\\\\n",
    "            .orderBy(desc(\"total_revenue\"))\n",
    "        \"\"\")\n",
    "        return None\n",
    "    \n",
    "    category_revenue = orders_df \\\n",
    "        .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "        .groupBy(\"category\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_orders\"),\n",
    "            sum(col(\"price\") * col(\"quantity\")).alias(\"total_revenue\"),\n",
    "            avg(\"price\").alias(\"avg_price\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    return category_revenue\n",
    "\n",
    "# Demonstracja agregacji na przykÅ‚adowych danych\n",
    "print(\"=== DEMONSTRACJA AGREGACJI STRUMIENIOWYCH ===\")\n",
    "print(\"\\nðŸ“Š 1. TOP PRODUKTY:\")\n",
    "create_top_products_aggregation(None)\n",
    "\n",
    "print(\"\\nðŸ“Š 2. KATEGORIE PRODUKTÃ“W:\")  \n",
    "create_category_aggregation(None)\n",
    "\n",
    "print(\"\\nâ±ï¸  KLUCZOWE CECHY AGREGACJI:\")\n",
    "print(\"- Watermark: 10 minut (tolerancja na spÃ³Åºnione dane)\")\n",
    "print(\"- Grupowanie: wedÅ‚ug product_id, product_name, category\")\n",
    "print(\"- Metryki: order_count, total_quantity, total_revenue\")\n",
    "print(\"- Sortowanie: wedÅ‚ug liczby zamÃ³wieÅ„ (desc)\")\n",
    "print(\"- State: zapisywany w checkpointach dla odpornoÅ›ci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ed3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaawansowane agregacje z oknem czasowym\n",
    "def create_windowed_aggregations(orders_df):\n",
    "    \"\"\"Agregacje w oknach czasowych - trendy sprzedaÅ¼y\"\"\"\n",
    "    if not orders_df:\n",
    "        print(\"ðŸ• OKNA CZASOWE - kod demonstracyjny:\")\n",
    "        print(\"\"\"\n",
    "        # Trendy sprzedaÅ¼y w oknie 15-minutowym (aktualizacja co 5 min)\n",
    "        windowed_sales = orders_df \\\\\n",
    "            .withWatermark(\"timestamp\", \"10 minutes\") \\\\\n",
    "            .groupBy(\n",
    "                window(col(\"timestamp\"), \"15 minutes\", \"5 minutes\"),\n",
    "                \"category\"\n",
    "            ) \\\\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"orders_in_window\"),\n",
    "                sum(col(\"price\") * col(\"quantity\")).alias(\"revenue_in_window\"),\n",
    "                countDistinct(\"product_id\").alias(\"unique_products\")\n",
    "            ) \\\\\n",
    "            .select(\n",
    "                col(\"window.start\").alias(\"window_start\"),\n",
    "                col(\"window.end\").alias(\"window_end\"), \n",
    "                col(\"category\"),\n",
    "                col(\"orders_in_window\"),\n",
    "                col(\"revenue_in_window\"),\n",
    "                col(\"unique_products\")\n",
    "            )\n",
    "        \"\"\")\n",
    "        return None\n",
    "    \n",
    "    # Rzeczywista implementacja\n",
    "    windowed_sales = orders_df \\\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"15 minutes\", \"5 minutes\"),\n",
    "            \"category\"\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"orders_in_window\"),\n",
    "            sum(col(\"price\") * col(\"quantity\")).alias(\"revenue_in_window\"),\n",
    "            countDistinct(\"product_id\").alias(\"unique_products\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            col(\"category\"),\n",
    "            col(\"orders_in_window\"), \n",
    "            col(\"revenue_in_window\"),\n",
    "            col(\"unique_products\")\n",
    "        )\n",
    "    \n",
    "    return windowed_sales\n",
    "\n",
    "# Symulacja wynikÃ³w na danych przykÅ‚adowych\n",
    "print(\"=== DEMONSTRACJA OKIEN CZASOWYCH ===\")\n",
    "create_windowed_aggregations(None)\n",
    "\n",
    "# PrzykÅ‚ad wynikÃ³w okien czasowych\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_time = datetime.now()\n",
    "sample_windows = [\n",
    "    {\n",
    "        'window_start': base_time - timedelta(minutes=15),\n",
    "        'window_end': base_time,\n",
    "        'category': 'Electronics', \n",
    "        'orders_in_window': 25,\n",
    "        'revenue_in_window': 3750.50,\n",
    "        'unique_products': 8\n",
    "    },\n",
    "    {\n",
    "        'window_start': base_time - timedelta(minutes=15),\n",
    "        'window_end': base_time,\n",
    "        'category': 'Clothing',\n",
    "        'orders_in_window': 18, \n",
    "        'revenue_in_window': 1620.25,\n",
    "        'unique_products': 5\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“ˆ PrzykÅ‚ad wynikÃ³w okien czasowych:\")\n",
    "df_windows = pd.DataFrame(sample_windows)\n",
    "for _, row in df_windows.iterrows():\n",
    "    print(f\"ðŸ• {row['window_start'].strftime('%H:%M')}-{row['window_end'].strftime('%H:%M')} | \"\n",
    "          f\"{row['category']}: {row['orders_in_window']} zamÃ³wieÅ„, \"\n",
    "          f\"${row['revenue_in_window']:.2f}, {row['unique_products']} produktÃ³w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205daa4",
   "metadata": {},
   "source": [
    "## 4. Podstawowe ETL: czyszczenie i transformacja danych zamÃ³wieÅ„\n",
    "\n",
    "**ETL (Extract, Transform, Load)** w kontekÅ›cie streamingu:\n",
    "- **Extract**: Odczyt danych z Kafki\n",
    "- **Transform**: Czyszczenie, walidacja, wzbogacanie danych  \n",
    "- **Load**: Zapis do sink (dashboard, baza danych, pliki)\n",
    "\n",
    "**Typowe transformacje w e-commerce**:\n",
    "- Walidacja pÃ³l obowiÄ…zkowych\n",
    "- Usuwanie duplikatÃ³w\n",
    "- Konwersja typÃ³w danych\n",
    "- Wyliczanie metryki biznesowych\n",
    "- Filtrowanie nieprawidÅ‚owych wartoÅ›ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107307fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcje ETL dla czyszczenia i transformacji danych\n",
    "from pyspark.sql.functions import col, when, isnan, isnull, regexp_replace, lower, trim\n",
    "\n",
    "def clean_and_validate_orders(orders_df):\n",
    "    \"\"\"CzyÅ›ci i waliduje dane zamÃ³wieÅ„\"\"\"\n",
    "    if not orders_df:\n",
    "        print(\"ðŸ§¹ FUNKCJE CZYSZCZENIA DANYCH - kod demonstracyjny:\")\n",
    "        print(\"\"\"\n",
    "        # 1. UsuniÄ™cie rekordÃ³w z brakujÄ…cymi wartoÅ›ciami kluczowymi\n",
    "        clean_orders = orders_df.filter(\n",
    "            col(\"order_id\").isNotNull() & \n",
    "            col(\"product_id\").isNotNull() &\n",
    "            col(\"price\").isNotNull() &\n",
    "            col(\"quantity\").isNotNull()\n",
    "        )\n",
    "        \n",
    "        # 2. Filtrowanie nieprawidÅ‚owych wartoÅ›ci biznesowych\n",
    "        clean_orders = clean_orders.filter(\n",
    "            (col(\"price\") > 0) & \n",
    "            (col(\"quantity\") > 0) &\n",
    "            (col(\"quantity\") <= 100)  # Maksymalnie 100 sztuk w zamÃ³wieniu\n",
    "        )\n",
    "        \n",
    "        # 3. Czyszczenie tekstu - normalizacja kategorii\n",
    "        clean_orders = clean_orders.withColumn(\n",
    "            \"category_clean\", \n",
    "            trim(lower(col(\"category\")))\n",
    "        )\n",
    "        \n",
    "        # 4. Wyliczanie metryki biznesowych\n",
    "        clean_orders = clean_orders.withColumn(\n",
    "            \"total_value\", \n",
    "            col(\"price\") * col(\"quantity\")\n",
    "        )\n",
    "        \"\"\")\n",
    "        return None\n",
    "    \n",
    "    # Rzeczywista implementacja\n",
    "    clean_orders = orders_df \\\n",
    "        .filter(\n",
    "            col(\"order_id\").isNotNull() & \n",
    "            col(\"product_id\").isNotNull() &\n",
    "            col(\"price\").isNotNull() &\n",
    "            col(\"quantity\").isNotNull()\n",
    "        ) \\\n",
    "        .filter(\n",
    "            (col(\"price\") > 0) & \n",
    "            (col(\"quantity\") > 0) &\n",
    "            (col(\"quantity\") <= 100)\n",
    "        ) \\\n",
    "        .withColumn(\"category_clean\", trim(lower(col(\"category\")))) \\\n",
    "        .withColumn(\"total_value\", col(\"price\") * col(\"quantity\"))\n",
    "    \n",
    "    return clean_orders\n",
    "\n",
    "def add_business_enrichments(orders_df):\n",
    "    \"\"\"Dodaje wzbogacenia biznesowe\"\"\"\n",
    "    if not orders_df:\n",
    "        print(\"\"\"\n",
    "        # Wzbogacenia biznesowe\n",
    "        enriched_orders = orders_df \\\\\n",
    "            .withColumn(\"price_category\", \n",
    "                when(col(\"price\") < 50, \"Budget\")\n",
    "                .when(col(\"price\") < 200, \"Mid-range\") \n",
    "                .otherwise(\"Premium\")\n",
    "            ) \\\\\n",
    "            .withColumn(\"order_hour\", hour(col(\"timestamp\"))) \\\\\n",
    "            .withColumn(\"is_weekend\", \n",
    "                when(dayofweek(col(\"timestamp\")).isin([1, 7]), True)\n",
    "                .otherwise(False)\n",
    "            )\n",
    "        \"\"\")\n",
    "        return None\n",
    "    \n",
    "    enriched_orders = orders_df \\\n",
    "        .withColumn(\"price_category\", \n",
    "            when(col(\"price\") < 50, \"Budget\")\n",
    "            .when(col(\"price\") < 200, \"Mid-range\")\n",
    "            .otherwise(\"Premium\")\n",
    "        ) \\\n",
    "        .withColumn(\"order_hour\", hour(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"is_weekend\", \n",
    "            when(dayofweek(col(\"timestamp\")).isin([1, 7]), True)\n",
    "            .otherwise(False)\n",
    "        )\n",
    "    \n",
    "    return enriched_orders\n",
    "\n",
    "# Demonstracja ETL na przykÅ‚adowych danych\n",
    "print(\"=== DEMONSTRACJA TRANSFORMACJI ETL ===\")\n",
    "print(\"\\nðŸ§¹ 1. CZYSZCZENIE I WALIDACJA:\")\n",
    "clean_and_validate_orders(None)\n",
    "\n",
    "print(\"\\nðŸ’° 2. WZBOGACENIA BIZNESOWE:\")\n",
    "add_business_enrichments(None)\n",
    "\n",
    "# PrzykÅ‚ad zastosowania na danych testowych\n",
    "sample_data = [\n",
    "    {\"order_id\": \"ORD-001\", \"product_id\": \"PROD-001\", \"price\": 299.99, \"quantity\": 1, \"category\": \" Electronics \", \"timestamp\": \"2024-01-15T14:30:00\"},\n",
    "    {\"order_id\": \"ORD-002\", \"product_id\": \"PROD-002\", \"price\": -50.0, \"quantity\": 2, \"category\": \"Clothing\", \"timestamp\": \"2024-01-15T14:31:00\"},  # NieprawidÅ‚owa cena\n",
    "    {\"order_id\": None, \"product_id\": \"PROD-003\", \"price\": 45.99, \"quantity\": 1, \"category\": \"Books\", \"timestamp\": \"2024-01-15T14:32:00\"},  # Brak order_id\n",
    "    {\"order_id\": \"ORD-004\", \"product_id\": \"PROD-001\", \"price\": 299.99, \"quantity\": 150, \"category\": \"Electronics\", \"timestamp\": \"2024-01-15T14:33:00\"},  # Za duÅ¼a iloÅ›Ä‡\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ“Š PrzykÅ‚ad przed czyszczeniem: {len(sample_data)} rekordÃ³w\")\n",
    "print(\"- 1 prawidÅ‚owy rekord\")  \n",
    "print(\"- 1 z nieprawidÅ‚owÄ… cenÄ… (-50.0)\")\n",
    "print(\"- 1 z brakujÄ…cym order_id\")\n",
    "print(\"- 1 z za duÅ¼Ä… iloÅ›ciÄ… (150)\")\n",
    "print(\"\\nâœ… Po czyszczeniu: zostanie 1 prawidÅ‚owy rekord\")\n",
    "print(\"âœ¨ Po wzbogaceniu: +price_category, +order_hour, +is_weekend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95dc939",
   "metadata": {},
   "source": [
    "## 5. Wizualizacja wynikÃ³w w dashboardzie (opcjonalnie)\n",
    "\n",
    "**System ASEED** zawiera kompleksowy dashboard z:\n",
    "- **Real-time metryki**: Å‚Ä…czne zamÃ³wienia, przychody, zamÃ³wienia/minutÄ™\n",
    "- **Wykresy interaktywne**: top produkty (Bar Chart), kategorie (Doughnut Chart)  \n",
    "- **Live data**: najnowsze zamÃ³wienia w czasie rzeczywistym\n",
    "- **WebSocket poÅ‚Ä…czenie**: natychmiastowe aktualizacje z Spark\n",
    "\n",
    "**Technologie**:\n",
    "- **Backend**: Flask + SocketIO + REST API\n",
    "- **Frontend**: Bootstrap + Chart.js + JavaScript\n",
    "- **Integracja**: Spark wysyÅ‚a dane przez HTTP POST do dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8821ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integracja z dashboardem ASEED\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def send_to_dashboard(endpoint, data, dashboard_url=\"http://localhost:5005\"):\n",
    "    \"\"\"WysyÅ‚a dane do dashboard ASEED\"\"\"\n",
    "    try:\n",
    "        response = requests.post(f\"{dashboard_url}/api/{endpoint}\", json=data, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… WysÅ‚ano dane do dashboard: {endpoint}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âš ï¸  BÅ‚Ä…d dashboard ({response.status_code}): {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Nie moÅ¼na poÅ‚Ä…czyÄ‡ z dashboard: {e}\")\n",
    "        return False\n",
    "\n",
    "# Symulacja danych dashboard\n",
    "def simulate_dashboard_data():\n",
    "    \"\"\"Symuluje dane ktÃ³re byÅ‚yby wysÅ‚ane do dashboard\"\"\"\n",
    "    \n",
    "    # Top produkty\n",
    "    top_products = [\n",
    "        {\"product_name\": \"Smart Watch Premium\", \"order_count\": 45, \"category\": \"Electronics\"},\n",
    "        {\"product_name\": \"Fashion Jacket\", \"order_count\": 32, \"category\": \"Clothing\"}, \n",
    "        {\"product_name\": \"Coffee Machine Pro\", \"order_count\": 28, \"category\": \"Home\"},\n",
    "        {\"product_name\": \"Running Shoes\", \"order_count\": 25, \"category\": \"Sports\"},\n",
    "        {\"product_name\": \"Python Book\", \"order_count\": 18, \"category\": \"Books\"}\n",
    "    ]\n",
    "    \n",
    "    # Kategorie\n",
    "    categories = [\n",
    "        {\"category\": \"Electronics\", \"total_revenue\": 8750.50, \"order_count\": 45},\n",
    "        {\"category\": \"Clothing\", \"total_revenue\": 3200.25, \"order_count\": 32},\n",
    "        {\"category\": \"Home\", \"total_revenue\": 5600.75, \"order_count\": 28},\n",
    "        {\"category\": \"Sports\", \"total_revenue\": 3250.00, \"order_count\": 25}, \n",
    "        {\"category\": \"Books\", \"total_revenue\": 810.50, \"order_count\": 18}\n",
    "    ]\n",
    "    \n",
    "    # Metryki ogÃ³lne\n",
    "    metrics = {\n",
    "        \"total_orders\": 148,\n",
    "        \"total_revenue\": 21611.00,\n",
    "        \"orders_per_minute\": 12\n",
    "    }\n",
    "    \n",
    "    return top_products, categories, metrics\n",
    "\n",
    "# Demonstracja z wizualizacjÄ…\n",
    "print(\"=== DEMONSTRACJA DASHBOARD ASEED ===\")\n",
    "\n",
    "# Pobierz przykÅ‚adowe dane\n",
    "top_products, categories, metrics = simulate_dashboard_data()\n",
    "\n",
    "print(f\"\\nðŸ“Š METRYKI OGÃ“LNE:\")\n",
    "print(f\"- ÅÄ…czne zamÃ³wienia: {metrics['total_orders']}\")\n",
    "print(f\"- ÅÄ…czny przychÃ³d: ${metrics['total_revenue']:,.2f}\")\n",
    "print(f\"- ZamÃ³wienia/minutÄ™: {metrics['orders_per_minute']}\")\n",
    "\n",
    "print(f\"\\nðŸ† TOP 3 PRODUKTY:\")\n",
    "for i, product in enumerate(top_products[:3], 1):\n",
    "    print(f\"{i}. {product['product_name']}: {product['order_count']} zamÃ³wieÅ„\")\n",
    "\n",
    "print(f\"\\nðŸ·ï¸  PRZYCHODY WEDÅUG KATEGORII:\")\n",
    "for category in categories:\n",
    "    print(f\"- {category['category']}: ${category['total_revenue']:,.2f}\")\n",
    "\n",
    "# Wizualizacja danych (podobnie jak w dashboard)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top produkty - Bar Chart\n",
    "products_names = [p['product_name'][:15] for p in top_products]\n",
    "products_counts = [p['order_count'] for p in top_products]\n",
    "\n",
    "ax1.bar(products_names, products_counts, color='skyblue', alpha=0.8)\n",
    "ax1.set_title('Top Produkty (liczba zamÃ³wieÅ„)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Produkt')\n",
    "ax1.set_ylabel('Liczba zamÃ³wieÅ„')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Kategorie - Pie Chart\n",
    "categories_names = [c['category'] for c in categories]\n",
    "categories_revenue = [c['total_revenue'] for c in categories]\n",
    "\n",
    "ax2.pie(categories_revenue, labels=categories_names, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Przychody wedÅ‚ug kategorii', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Aby uruchomiÄ‡ rzeczywisty dashboard:\")\n",
    "print(\"   python3 aseed.py start\")\n",
    "print(\"   OtwÃ³rz: http://localhost:5005\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86273af1",
   "metadata": {},
   "source": [
    "## 6. Testowanie i walidacja pipeline'u streamingowego\n",
    "\n",
    "**Testy w systemach streamingowych** wymagajÄ… specjalnego podejÅ›cia:\n",
    "- **Unit testy**: pojedyncze funkcje transformacji\n",
    "- **Integration testy**: end-to-end pipeline  \n",
    "- **Performance testy**: przepustowoÅ›Ä‡ i latencja\n",
    "- **Data quality testy**: poprawnoÅ›Ä‡ agregacji\n",
    "\n",
    "**NarzÄ™dzia testowe**:\n",
    "- **pytest**: framework testowy dla Python\n",
    "- **Spark Testing Base**: narzÄ™dzia do testowania Spark\n",
    "- **Testcontainers**: izolowane Å›rodowiska testowe (Kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ady testÃ³w dla pipeline ASEED\n",
    "import unittest\n",
    "from datetime import datetime\n",
    "\n",
    "class TestASEEDPipeline(unittest.TestCase):\n",
    "    \"\"\"Testy jednostkowe dla funkcji ETL\"\"\"\n",
    "    \n",
    "    def test_order_validation(self):\n",
    "        \"\"\"Test walidacji zamÃ³wieÅ„\"\"\"\n",
    "        # PrzykÅ‚adowe dane testowe\n",
    "        valid_order = {\n",
    "            \"order_id\": \"ORD-123\", \n",
    "            \"product_id\": \"PROD-001\",\n",
    "            \"price\": 99.99,\n",
    "            \"quantity\": 2\n",
    "        }\n",
    "        \n",
    "        invalid_orders = [\n",
    "            {\"order_id\": None, \"product_id\": \"PROD-001\", \"price\": 99.99, \"quantity\": 2},  # Brak ID\n",
    "            {\"order_id\": \"ORD-124\", \"product_id\": \"PROD-001\", \"price\": -10, \"quantity\": 2},  # Ujemna cena\n",
    "            {\"order_id\": \"ORD-125\", \"product_id\": \"PROD-001\", \"price\": 99.99, \"quantity\": 0},  # Zero iloÅ›Ä‡\n",
    "        ]\n",
    "        \n",
    "        print(\"ðŸ§ª Test walidacji zamÃ³wieÅ„:\")\n",
    "        print(f\"âœ… PrawidÅ‚owe zamÃ³wienie: {valid_order['order_id']}\")\n",
    "        \n",
    "        for i, order in enumerate(invalid_orders, 1):\n",
    "            reason = \"brak order_id\" if not order.get(\"order_id\") else \\\n",
    "                    \"ujemna cena\" if order.get(\"price\", 0) < 0 else \\\n",
    "                    \"zero iloÅ›Ä‡\"\n",
    "            print(f\"âŒ NieprawidÅ‚owe zamÃ³wienie {i}: {reason}\")\n",
    "        \n",
    "        # W rzeczywistym teÅ›cie:\n",
    "        # self.assertTrue(validate_order(valid_order))\n",
    "        # for invalid_order in invalid_orders:\n",
    "        #     self.assertFalse(validate_order(invalid_order))\n",
    "    \n",
    "    def test_aggregation_logic(self):\n",
    "        \"\"\"Test logiki agregacji\"\"\"\n",
    "        # PrzykÅ‚adowe zamÃ³wienia dla tego samego produktu\n",
    "        orders = [\n",
    "            {\"product_id\": \"PROD-001\", \"quantity\": 2, \"price\": 100},\n",
    "            {\"product_id\": \"PROD-001\", \"quantity\": 1, \"price\": 100},\n",
    "            {\"product_id\": \"PROD-002\", \"quantity\": 3, \"price\": 50}\n",
    "        ]\n",
    "        \n",
    "        # Oczekiwane wyniki agregacji\n",
    "        expected_results = {\n",
    "            \"PROD-001\": {\"order_count\": 2, \"total_quantity\": 3, \"total_revenue\": 300},\n",
    "            \"PROD-002\": {\"order_count\": 1, \"total_quantity\": 3, \"total_revenue\": 150}\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸ§ª Test agregacji:\")\n",
    "        print(f\"ðŸ“ Dane wejÅ›ciowe: {len(orders)} zamÃ³wieÅ„\")\n",
    "        \n",
    "        for product_id, expected in expected_results.items():\n",
    "            print(f\"âœ… {product_id}: {expected['order_count']} zamÃ³wieÅ„, \"\n",
    "                  f\"{expected['total_quantity']} sztuk, ${expected['total_revenue']} przychÃ³d\")\n",
    "        \n",
    "        # W rzeczywistym teÅ›cie:\n",
    "        # actual_results = aggregate_orders(orders)\n",
    "        # self.assertEqual(actual_results, expected_results)\n",
    "\n",
    "def test_data_quality():\n",
    "    \"\"\"Test jakoÅ›ci danych - sprawdzenie poprawnoÅ›ci wynikÃ³w\"\"\"\n",
    "    \n",
    "    # Symulacja danych z systemu\n",
    "    system_metrics = {\n",
    "        \"total_orders\": 150,\n",
    "        \"total_revenue\": 12750.50,\n",
    "        \"unique_products\": 15,\n",
    "        \"avg_order_value\": 85.00\n",
    "    }\n",
    "    \n",
    "    print(\"\\nðŸ” Test jakoÅ›ci danych:\")\n",
    "    \n",
    "    # Test 1: SpÃ³jnoÅ›Ä‡ metryk\n",
    "    calculated_avg = system_metrics[\"total_revenue\"] / system_metrics[\"total_orders\"]\n",
    "    avg_diff = abs(calculated_avg - system_metrics[\"avg_order_value\"])\n",
    "    \n",
    "    if avg_diff < 0.01:  # Tolerancja na bÅ‚Ä™dy zaokrÄ…glenia\n",
    "        print(\"âœ… Åšrednia wartoÅ›Ä‡ zamÃ³wienia jest spÃ³jna\")\n",
    "    else:\n",
    "        print(f\"âŒ NiespÃ³jnoÅ›Ä‡ w Å›redniej wartoÅ›ci: {avg_diff:.2f}\")\n",
    "    \n",
    "    # Test 2: Logiczne granice\n",
    "    checks = [\n",
    "        (\"total_orders\", system_metrics[\"total_orders\"] > 0, \"Liczba zamÃ³wieÅ„ > 0\"),  \n",
    "        (\"total_revenue\", system_metrics[\"total_revenue\"] > 0, \"PrzychÃ³d > 0\"),\n",
    "        (\"unique_products\", system_metrics[\"unique_products\"] <= system_metrics[\"total_orders\"], \n",
    "         \"Produkty <= zamÃ³wienia\"),\n",
    "        (\"avg_order_value\", 0 < system_metrics[\"avg_order_value\"] < 10000, \n",
    "         \"Åšrednia wartoÅ›Ä‡ w rozsÄ…dnych granicach\")\n",
    "    ]\n",
    "    \n",
    "    for field, condition, description in checks:\n",
    "        if condition:\n",
    "            print(f\"âœ… {description}\")\n",
    "        else:\n",
    "            print(f\"âŒ {description} - FAILED\")\n",
    "\n",
    "def run_performance_test():\n",
    "    \"\"\"Symulacja testu wydajnoÅ›ci\"\"\"\n",
    "    print(\"\\nâš¡ Test wydajnoÅ›ci pipeline:\")\n",
    "    \n",
    "    # Symulacja metryk wydajnoÅ›ci\n",
    "    performance_metrics = {\n",
    "        \"throughput_orders_per_sec\": 50,\n",
    "        \"avg_processing_latency_ms\": 100,\n",
    "        \"memory_usage_mb\": 256,\n",
    "        \"cpu_usage_percent\": 45\n",
    "    }\n",
    "    \n",
    "    # Progi wydajnoÅ›ci\n",
    "    thresholds = {\n",
    "        \"min_throughput\": 30,\n",
    "        \"max_latency_ms\": 500, \n",
    "        \"max_memory_mb\": 512,\n",
    "        \"max_cpu_percent\": 80\n",
    "    }\n",
    "    \n",
    "    for metric, value in performance_metrics.items():\n",
    "        threshold_key = None\n",
    "        if \"throughput\" in metric:\n",
    "            threshold_key = \"min_throughput\"\n",
    "            passed = value >= thresholds[threshold_key]\n",
    "        elif \"latency\" in metric:\n",
    "            threshold_key = \"max_latency_ms\"\n",
    "            passed = value <= thresholds[threshold_key]\n",
    "        elif \"memory\" in metric:\n",
    "            threshold_key = \"max_memory_mb\"\n",
    "            passed = value <= thresholds[threshold_key]\n",
    "        elif \"cpu\" in metric:\n",
    "            threshold_key = \"max_cpu_percent\"\n",
    "            passed = value <= thresholds[threshold_key]\n",
    "        \n",
    "        status = \"âœ…\" if passed else \"âŒ\"\n",
    "        print(f\"{status} {metric}: {value} (prÃ³g: {thresholds[threshold_key]})\")\n",
    "\n",
    "# Uruchomienie testÃ³w demonstracyjnych\n",
    "print(\"=== DEMONSTRACJA TESTÃ“W ASEED PIPELINE ===\")\n",
    "\n",
    "# Testy jednostkowe\n",
    "test_suite = TestASEEDPipeline()\n",
    "test_suite.test_order_validation()\n",
    "test_suite.test_aggregation_logic()\n",
    "\n",
    "# Testy jakoÅ›ci danych\n",
    "test_data_quality()\n",
    "\n",
    "# Test wydajnoÅ›ci  \n",
    "run_performance_test()\n",
    "\n",
    "print(\"\\nðŸ’¡ Aby uruchomiÄ‡ peÅ‚ne testy:\")\n",
    "print(\"   cd /home/natan/ASEED\")\n",
    "print(\"   python -m pytest tests/ -v\")\n",
    "print(\"   python aseed.py test --minutes 2 --rate 30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab2ff3",
   "metadata": {},
   "source": [
    "## Podsumowanie i wnioski akademickie\n",
    "\n",
    "### âœ… SpeÅ‚nienie wymagaÅ„ projektu\n",
    "\n",
    "**ZgodnoÅ›Ä‡ z poleceniem**:\n",
    "- âœ… **Kafka topic**: zamÃ³wienia z wymaganymi polami (order_id, product_id, price, timestamp)\n",
    "- âœ… **Spark Structured Streaming**: analiza top produktÃ³w w czasie rzeczywistym\n",
    "- âœ… **Streaming aggregations**: grupowanie, liczenie, sumowanie w oknach czasowych\n",
    "- âœ… **ETL patterns**: extract (Kafka) â†’ transform (czyszczenie) â†’ load (dashboard)\n",
    "\n",
    "### ðŸŽ¯ Cele edukacyjne osiÄ…gniÄ™te\n",
    "\n",
    "**1. Praktyka streaming aggregations**:\n",
    "- Agregacje grupowe (`groupBy`, `agg`)\n",
    "- Funkcje okna czasowego (`window`, `watermark`) \n",
    "- Stateful operations z checkpointingiem\n",
    "\n",
    "**2. Podstawowe wzorce ETL**:\n",
    "- Walidacja i czyszczenie danych w czasie rzeczywistym\n",
    "- Transformacje biznesowe (wyliczanie wartoÅ›ci, kategoryzacja)\n",
    "- Pipeline data quality z monitoringiem\n",
    "\n",
    "**3. Architektura mikrousÅ‚ug**:\n",
    "- Rozdzielenie odpowiedzialnoÅ›ci (simulator, processor, dashboard)\n",
    "- Asynchroniczna komunikacja przez Kafka\n",
    "- REST API i WebSocket dla real-time UI\n",
    "\n",
    "### ðŸ“Š WartoÅ›Ä‡ biznesowa\n",
    "\n",
    "**Metryki monitorowane**:\n",
    "- Top produkty wedÅ‚ug liczby zamÃ³wieÅ„\n",
    "- Przychody wedÅ‚ug kategorii produktÃ³w  \n",
    "- Trendy sprzedaÅ¼y w oknach czasowych\n",
    "- WskaÅºniki wydajnoÅ›ci (zamÃ³wienia/minutÄ™)\n",
    "\n",
    "**Przypadki uÅ¼ycia**:\n",
    "- Wykrywanie trendÃ³w sprzedaÅ¼owych w czasie rzeczywistym\n",
    "- Optymalizacja zarzÄ…dzania zapasami\n",
    "- Personalizacja rekomendacji produktÃ³w\n",
    "- Monitoring wydajnoÅ›ci systemu e-commerce\n",
    "\n",
    "### ðŸ”§ Aspekty techniczne\n",
    "\n",
    "**Zalety architektury**:\n",
    "- **SkalowalnoÅ›Ä‡**: Kafka partitioning + Spark parallelization\n",
    "- **OdpornoÅ›Ä‡**: Checkpointing + fault tolerance\n",
    "- **ElastycznoÅ›Ä‡**: Modularna architektura z wymienialnymi komponentami\n",
    "- **Monitoring**: Logi, metryki, dashboard w czasie rzeczywistym\n",
    "\n",
    "**MoÅ¼liwe rozszerzenia**:\n",
    "- Machine Learning (predykcja sprzedaÅ¼y, anomalie)\n",
    "- WiÄ™ksza liczba sink'Ã³w (bazy danych, analytics platforms)\n",
    "- Complex event processing (wzorce zachowaÅ„ klientÃ³w)\n",
    "- Auto-scaling na podstawie load'u\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Uruchomienie systemu ASEED\n",
    "\n",
    "```bash\n",
    "# Sklonuj i zainstaluj\n",
    "git clone https://github.com/NatanTulo/ASEED.git\n",
    "cd ASEED\n",
    "./install.sh\n",
    "\n",
    "# Uruchom wszystkie komponenty\n",
    "python3 aseed.py start\n",
    "\n",
    "# OtwÃ³rz dashboard\n",
    "http://localhost:5005\n",
    "\n",
    "# Test z danymi\n",
    "python3 aseed.py test --minutes 5 --rate 20\n",
    "```\n",
    "\n",
    "**System gotowy do prezentacji i oceny! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
